{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "The initialization is performed in 3 steps:\n",
    "* Import the Spark and the PostgreSQL driver libraries\n",
    "* Import the TDM library\n",
    "* Setup of the database parameters (to be modified according to your installation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T12:37:22.062917Z",
     "start_time": "2020-02-26T12:37:18.400Z"
    }
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.4`\n",
    "import $ivy.`org.postgresql:postgresql:42.2.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T12:37:25.644140Z",
     "start_time": "2020-02-26T12:37:24.420Z"
    }
   },
   "outputs": [],
   "source": [
    "val path = java.nio.file.FileSystems.getDefault().getPath(new java.io.File(\".\").getCanonicalPath + \"/TDM-assembly-0.2.0.jar\")\n",
    "val x = ammonite.ops.Path(path)\n",
    "interp.load.cp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T12:43:58.529223Z",
     "start_time": "2020-02-26T12:43:58.107Z"
    }
   },
   "outputs": [],
   "source": [
    "val user: String = \"user\"\n",
    "val password: String = \"password\"\n",
    "val url: String = \"jdbc:postgresql://localhost:5432/db\"\n",
    "\n",
    "val tweetTable: String = \"tweet\"\n",
    "val hashtagTable: String = \"tweet_hashtag\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data\n",
    "Two anonymized CSV files are provided to reproduce the experiment. This step consists in creating the tables and inserting the data in your database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T12:44:00.414841Z",
     "start_time": "2020-02-26T12:43:59.957Z"
    }
   },
   "outputs": [],
   "source": [
    "import java.sql._\n",
    "import java.util.Properties\n",
    "\n",
    "Class.forName(\"org.postgresql.Driver\")\n",
    "val props: Properties = new Properties()\n",
    "props.setProperty(\"user\", user)\n",
    "props.setProperty(\"password\", password)\n",
    "\n",
    "val connection: Connection = DriverManager.getConnection(url, props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T12:56:05.494042Z",
     "start_time": "2020-02-26T12:56:05.029Z"
    }
   },
   "outputs": [],
   "source": [
    "val createTableTweet: String = s\"\"\"\n",
    "    CREATE TABLE ${tweetTable} (\n",
    "        id TEXT PRIMARY KEY,\n",
    "        from_user_id TEXT,\n",
    "        time BIGINT\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "val createTableHashtag: String = s\"\"\"\n",
    "    CREATE TABLE ${hashtagTable} (\n",
    "        tweet_id TEXT,\n",
    "        hashtag TEXT,\n",
    "        PRIMARY KEY (tweet_id, hashtag)\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "val stmt: Statement = connection.createStatement()\n",
    "stmt.execute(createTableTweet)\n",
    "stmt.execute(createTableHashtag)\n",
    "stmt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T13:24:25.014184Z",
     "start_time": "2020-02-26T13:22:02.526Z"
    }
   },
   "outputs": [],
   "source": [
    "import java.io.FileReader\n",
    "import org.postgresql.copy.CopyManager\n",
    "import org.postgresql.core.BaseConnection\n",
    "\n",
    "val cm: CopyManager = new CopyManager(connection.asInstanceOf[BaseConnection])\n",
    "cm.copyIn(s\"COPY ${tweetTable} FROM STDIN\", new FileReader(\"tweets_part1.csv\"))\n",
    "cm.copyIn(s\"COPY ${tweetTable} FROM STDIN\", new FileReader(\"tweets_part2.csv\"))\n",
    "cm.copyIn(s\"COPY ${tweetTable} FROM STDIN\", new FileReader(\"tweets_part3.csv\"))\n",
    "cm.copyIn(s\"COPY ${hashtagTable} FROM STDIN\", new FileReader(\"hashtags_part1.csv\"))\n",
    "cm.copyIn(s\"COPY ${hashtagTable} FROM STDIN\", new FileReader(\"hashtags_part2.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T15:52:48.882390Z",
     "start_time": "2020-02-10T15:52:05.209Z"
    }
   },
   "source": [
    "# Experiment\n",
    "After having created the tables and having filled them, we carry out two experiments, each run with Spark and TDM: \n",
    "* The first one to mesure the global execution time by forcing the computation of operators at each step,\n",
    "* The second one to mesure the global execution time by forcing the computation of operators only at the last step.\n",
    "\n",
    "Before launching the experiments for different volumes of data, it is necessary to define the general parameters: \n",
    "* Dimensions of the tensors,\n",
    "* Queries to be run against the database,\n",
    "* SparkSession initialization.\n",
    "\n",
    "Starting from 2 tensors:\n",
    "* U of order 1 that contains user id as dimension and the number of published tweets by each user as tensor's values,\n",
    "* UHT of order 3 that contains user id, hashtag and time as dimensions, and the number of times a user has used a hashtag per time slice (1 hour).\n",
    "\n",
    "We apply 2 operators:\n",
    "* A selection on U to keep users who have published at least 100 tweets,\n",
    "* A natural join between UHT and U to keep only the most active users.\n",
    "\n",
    "In the following experiements, we vary the size of U from 0 to 10^6 by step of 100 000, and we repeat each execution 5 times to produce an average time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T11:01:34.654473Z",
     "start_time": "2020-02-21T11:01:31.458Z"
    }
   },
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T20:24:38.279969Z",
     "start_time": "2020-02-10T20:24:37.359Z"
    }
   },
   "outputs": [],
   "source": [
    "import tdm._\n",
    "import tdm.core._\n",
    "import java.util.Properties\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = {\n",
    "  SparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "val props: Properties = new Properties()\n",
    "props.setProperty(\"user\", user)\n",
    "props.setProperty(\"password\", password)\n",
    "props.setProperty(\"url\", url)\n",
    "\n",
    "object User extends TensorDimension[String]\n",
    "object Hashtag extends TensorDimension[String]\n",
    "object Time extends TensorDimension[Long]\n",
    "object Retweet extends TensorDimension[String]\n",
    "\n",
    "val queryUser = s\"\"\"\n",
    "  SELECT DISTINCT from_user_id AS user, COUNT(*)::INTEGER AS freq\n",
    "    FROM ${tweetTable} t \n",
    "    GROUP BY from_user_id\n",
    "  \"\"\"\n",
    "\n",
    "val queryUserHashtagTime = s\"\"\"\n",
    "  SELECT t.from_user_id AS u, ht.hashtag AS ht, FLOOR(t.time / (3600)) AS t, COUNT(*)::INTEGER AS freq\n",
    "    FROM ${tweetTable} t INNER JOIN\n",
    "        ${hashtagTable} ht ON t.id = ht.tweet_id\n",
    "    GROUP BY u, ht, t\n",
    "    HAVING COUNT(*) > 5\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Force compute at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T10:30:11.454352Z",
     "start_time": "2020-02-11T09:12:33.048Z"
    }
   },
   "outputs": [],
   "source": [
    "var timeTDMForceCompute = Seq[Int]()\n",
    "\n",
    "for (i <- 0 to 1000000 by 100000) {\n",
    "    val nbIterations = 5\n",
    "    var endTime = 0\n",
    "    println(i)\n",
    "    for (j <- 0 until nbIterations) {\n",
    "        val startTime = System.currentTimeMillis()\n",
    "\n",
    "        val tensorUserAll = TensorBuilder[Int](props)\n",
    "            .addDimension(User, \"user\")\n",
    "            .build(queryUser + \" LIMIT \" + i, \"freq\")\n",
    "\n",
    "        val tensorUserHashtagTime = TensorBuilder[Int](props)\n",
    "            .addDimension(User, \"u\")\n",
    "            .addDimension(Hashtag, \"ht\")\n",
    "            .addDimension(Time, \"t\")\n",
    "            .build(queryUserHashtagTime, \"freq\")\n",
    "        tensorUserHashtagTime.count()\n",
    "\n",
    "        val tensorUser = tensorUserAll.selection(_ > 100)\n",
    "        tensorUser.count()\n",
    "\n",
    "        val tensorFinal = tensorUserHashtagTime.naturalJoin(tensorUser)\n",
    "        tensorFinal.count()\n",
    "\n",
    "        endTime += (System.currentTimeMillis() - startTime).toInt\n",
    "\n",
    "        println(\"Execution time: \" + (endTime / 1000) + \"s\")\n",
    "    }\n",
    "    timeTDMForceCompute = timeTDMForceCompute :+ (endTime / nbIterations).toInt \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T11:47:23.193360Z",
     "start_time": "2020-02-11T09:14:17.950Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val jdbcReader = spark.read.format(\"jdbc\")\n",
    "props.forEach((key, value) => jdbcReader.option(key.toString, value.toString))\n",
    "\n",
    "var timeSparkForceCompute = Seq[Int]()\n",
    "\n",
    "for (i <- 0 to 1000000 by 100000) {\n",
    "    val nbIterations = 5\n",
    "    var endTime = 0\n",
    "    println(i)\n",
    "    for (j <- 0 until nbIterations) {\n",
    "        val startTime = System.currentTimeMillis()\n",
    "\n",
    "        val dfUserAll = jdbcReader.option(\"query\", queryUser + \" LIMIT \" + i).load() // user - freq\n",
    "        dfUserAll.count()\n",
    "\n",
    "        val dfUserHashtagTime = jdbcReader.option(\"query\", queryUserHashtagTime).load() // u - ht - t - freq\n",
    "        dfUserHashtagTime.count()\n",
    "\n",
    "        val index = dfUserAll.columns.indexOf(\"freq\")\n",
    "        val dfUser = dfUserAll.filter(r => r.get(index).asInstanceOf[Int] > 100)\n",
    "        dfUser.count()\n",
    "\n",
    "        val dfFinal = dfUserHashtagTime.join(dfUser.drop(\"freq\"), col(\"u\") === col(\"user\"))\n",
    "        dfFinal.count()\n",
    "\n",
    "        endTime += (System.currentTimeMillis() - startTime).toInt\n",
    "\n",
    "        println(\"Execution time: \" + (endTime / 1000) + \"s\")\n",
    "    }\n",
    "    \n",
    "    timeSparkForceCompute = timeSparkForceCompute :+ (endTime / nbIterations).toInt \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Force compute only at the last step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T12:56:57.644445Z",
     "start_time": "2020-02-11T09:14:24.742Z"
    }
   },
   "outputs": [],
   "source": [
    "var timeTDM = Seq[Int]()\n",
    "\n",
    "for (i <- 0 to 1000000 by 100000) {\n",
    "    println(i)\n",
    "    val nbIterations = 5\n",
    "    var endTime = 0\n",
    "    for (j <- 0 until nbIterations) {\n",
    "        val startTime = System.currentTimeMillis()\n",
    "\n",
    "        val tensorUserAll = TensorBuilder[Int](props)\n",
    "            .addDimension(User, \"user\")\n",
    "            .build(queryUser + \" LIMIT \" + i, \"freq\")\n",
    "\n",
    "        val tensorUserHashtagTime = TensorBuilder[Int](props)\n",
    "            .addDimension(User, \"u\")\n",
    "            .addDimension(Hashtag, \"ht\")\n",
    "            .addDimension(Time, \"t\")\n",
    "            .build(queryUserHashtagTime, \"freq\")\n",
    "\n",
    "        val tensorUser = tensorUserAll.selection(_ > 100)\n",
    "\n",
    "        val tensorFinal = tensorUserHashtagTime.naturalJoin(tensorUser)\n",
    "        tensorFinal.count()\n",
    "\n",
    "        endTime += (System.currentTimeMillis() - startTime).toInt\n",
    "\n",
    "        println(\"Execution time: \" + (endTime / 1000) + \"s\")\n",
    "    }\n",
    "    \n",
    "    timeTDM = timeTDM :+ (endTime / nbIterations).toInt \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T12:21:52.876876Z",
     "start_time": "2020-02-11T09:14:20.151Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val jdbcReader = spark.read.format(\"jdbc\")\n",
    "props.forEach((key, value) => jdbcReader.option(key.toString, value.toString))\n",
    "\n",
    "var timeSpark = Seq[Int]()\n",
    "\n",
    "for (i <- 0 to 1000000 by 100000) {\n",
    "    println(i)\n",
    "    val nbIterations = 5\n",
    "    var endTime = 0\n",
    "    for (j <- 0 until nbIterations) {\n",
    "        val startTime = System.currentTimeMillis()\n",
    "\n",
    "        val dfUserAll = jdbcReader.option(\"query\", queryUser + \" LIMIT \" + i).load() // user - freq\n",
    "\n",
    "        val dfUserHashtagTime = jdbcReader.option(\"query\", queryUserHashtagTime).load() // u - ht - t - freq\n",
    "\n",
    "        val index = dfUserAll.columns.indexOf(\"freq\")\n",
    "        val dfUser = dfUserAll.filter(r => r.get(index).asInstanceOf[Int] > 100)\n",
    "\n",
    "        val dfFinal = dfUserHashtagTime.join(dfUser.drop(\"freq\"), col(\"u\") === col(\"user\"))\n",
    "        dfFinal.count()\n",
    "\n",
    "        endTime += (System.currentTimeMillis() - startTime).toInt\n",
    "\n",
    "        println(\"Execution time: \" + (endTime / 1000) + \"s\")\n",
    "    }\n",
    "    \n",
    "    timeSpark = timeSpark :+ (endTime / nbIterations).toInt \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "For the 4 cases (experiments defined in section 3), we summarize the results obtained with curves showing the evolution of the execution time following the size of the tensor U."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T17:38:25.822155Z",
     "start_time": "2020-02-12T17:38:20.848Z"
    }
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.plotly-scala::plotly-almond:0.7.2`\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.Almond._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T17:30:41.984776Z",
     "start_time": "2020-02-11T17:30:41.698Z"
    }
   },
   "outputs": [],
   "source": [
    "val x = 0 to 1000000 by 100000\n",
    "\n",
    "val plot = Seq(\n",
    "    Scatter(\n",
    "        x, timeTDM.map(_ / 1000), name = \"Execution with TDM\"\n",
    "    ),\n",
    "    Scatter(\n",
    "        x, timeSpark.map(_ / 1000), name = \"Execution with Spark\"\n",
    "    ),\n",
    "    Scatter(\n",
    "        x, timeTDMForceCompute.map(_ / 1000), name = \"Forced execution with TDM\"\n",
    "    ),\n",
    "    Scatter(\n",
    "        x, timeSparkForceCompute.map(_ / 1000), name = \"Forced execution with Spark\"\n",
    "    )\n",
    ")\n",
    "\n",
    "plot.plot(title = \"Execution time\", \n",
    "          xaxis = Axis(title = \"Size of Tensor U\"),\n",
    "          yaxis = Axis(title = \"Time (s)\", range = (0.0, 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
